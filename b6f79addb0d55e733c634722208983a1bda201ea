{
  "comments": [
    {
      "key": {
        "uuid": "07f24f33_47982701",
        "filename": "pages/design-docs/scaling-multi-master-replication/index.md",
        "patchSetId": 5
      },
      "lineNbr": 12,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-22T21:04:13Z",
      "side": 1,
      "message": "Have you thought about other solutions not using a shared filesystem?",
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d6aab8c4_4199c8bf",
        "filename": "pages/design-docs/scaling-multi-master-replication/index.md",
        "patchSetId": 5
      },
      "lineNbr": 12,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-22T21:30:31Z",
      "side": 1,
      "message": "I discuss several alternatives in the doc, please take a look.",
      "parentUuid": "07f24f33_47982701",
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "89eedb7d_1bf21f99",
        "filename": "pages/design-docs/scaling-multi-master-replication/index.md",
        "patchSetId": 5
      },
      "lineNbr": 12,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-22T21:37:27Z",
      "side": 1,
      "message": "The only one I\u0027ve found is:\n\"Since the current filesystem based storage exists and can be enhanced fairly easily to satisfy the desired use case as a polling based sharing solution, the extra expense of other sharing solutions does not currently seem worth considering.\"\n\nCan you point me to the exact section where you analyse a non-shared filesystem based alternative for distributing the replication load across masters?",
      "parentUuid": "d6aab8c4_4199c8bf",
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "39ec1d82_f93e93f0",
        "filename": "pages/design-docs/scaling-multi-master-replication/index.md",
        "patchSetId": 5
      },
      "lineNbr": 12,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-22T21:55:25Z",
      "side": 1,
      "message": "That is what I am referring to. Please comment there if you don\u0027t understand or have more specific questions.",
      "parentUuid": "89eedb7d_1bf21f99",
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4f697336_4fc8e80c",
        "filename": "pages/design-docs/scaling-multi-master-replication/index.md",
        "patchSetId": 5
      },
      "lineNbr": 12,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-22T22:01:31Z",
      "side": 1,
      "message": "OK, will comment there.",
      "parentUuid": "39ec1d82_f93e93f0",
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "cbe030ac_24945f56",
        "filename": "pages/design-docs/scaling-multi-master-replication/solution-distribute-replication-tasks-via-shared-FS.md",
        "patchSetId": 5
      },
      "lineNbr": 283,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-22T22:01:31Z",
      "side": 1,
      "message": "The shared filesystem storage has problems too:\n- latency\n- concurrent locking\n- stale file handles\n- caching issues\n- lack of real-time notifications\n\nYou are basically saying that you did not believe it was worth to think about any other solution other than reusing the shared filesystem, just because it is there.\n\nI understand the point, however, it goes against the design-driven approach IMHO, where we asked whoever proposed a solution to think about other possible solutions.",
      "range": {
        "startLine": 282,
        "startChar": 54,
        "endLine": 283,
        "endChar": 41
      },
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "852d4965_c80197b5",
        "filename": "pages/design-docs/scaling-multi-master-replication/solution-distribute-replication-tasks-via-shared-FS.md",
        "patchSetId": 5
      },
      "lineNbr": 283,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-22T22:58:48Z",
      "side": 1,
      "message": "\u003e The shared filesystem storage has problems too:\n\n\u003e - latency\n\nI am assuming this is not a repeat of the concern below about \"real-time\", so perhaps it refers to persistence latency, and or directory listing latency? It likely has lower or comparable persistence latency to most other solutions. For this use case, the persistence latency does not seem to specifically be very important. As for directory listings, I believe there are many ways to improve the design via sharding and file attributes to decrease this latency if this becomes an issue. The lack of sharding is likely already a replication persistence problem with the current plugin and likely should be dealt with even without this solution.\n\n\u003e - concurrent locking\n\nI believe for this simple use case this likely is on par with other solutions, see the locking prototype here: https://gerrit-review.googlesource.com/c/plugins/replication/+/241132\n\n\u003e - stale file handles\n\nAside from potentially interrupting code execution, these are likely not disruptive since they are effectively file not founds which can easily be handled.\n\n\u003e - caching issues\n\nI believe that many of the current Gerrit NFS caching issues are due to the jgit caching layers which are hard to workaround.\n\nWithout jgit in the way these should not be a big challenge. The files are currently immutable so they have no caching issues. In my experience refreshing parent directories will refresh any operation that uses directory listings on the child directory. It probably would be good (and easy) to build that into the plugin.\n\n\u003e - lack of real-time notifications\n\nI think you mean event vs polling (since real-time and Gerrit are not really compatible)? I address this in the next section.\n\n\u003e You are basically saying that you did not believe it was worth to think about any other solution other than reusing the shared filesystem, just because it is there.\n\nI am saying that other storage mechanism have extra expenses and the expense list is huge. Here are some obvious expenses: developing a new persistent mechanism, adding external tools to maintain, likely lower reliability due to extra moving parts, likely more communication overhead, likely more libraries to manage with more version compatibility issues, likely more expertise needed to manage, likely more RAM usage, and the list goes on...\n\n\u003e I understand the point, however, it goes against the design-driven approach IMHO, where we asked whoever proposed a solution to think about other possible solutions.\n\nI believe I listed about 6 of them. It is true that I have not listed any explicit alternate sharing mechanisms as all the ones I can think of would suffer the extra expenses that I mention. Do you have any specific suggestions that you believe do not suffer extra expenses?",
      "parentUuid": "cbe030ac_24945f56",
      "range": {
        "startLine": 282,
        "startChar": 54,
        "endLine": 283,
        "endChar": 41
      },
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "a96e654d_7d9e9ee7",
        "filename": "pages/design-docs/scaling-multi-master-replication/solution-distribute-replication-tasks-via-shared-FS.md",
        "patchSetId": 5
      },
      "lineNbr": 283,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-24T09:59:48Z",
      "side": 1,
      "message": "\u003e \u003e The shared filesystem storage has problems too:\n\u003e \n\u003e \u003e - latency\n\u003e \n\u003e I am assuming this is not a repeat of the concern below about \"real-time\", so perhaps it refers to persistence latency, and or directory listing latency? \n\nWith NFS, the other nodes could see new files seconds or even minutes afterwards. Adding 1-2 mins delay to all existing replications could be a serious issue.\n\nIn our use-case, we use HA and multi-site, we want to reduce the replication latency from tens of seconds to less than 1 sec: this solution won\u0027t be helpful.\n\nUsing other shared replication queue implementation (e.g. a message broker) would resolve this problem and reduce latency to msecs.\n\nWhy not making the replication queue storage pluggable with a DynamicItem? The filesystem-based implementation, that would be best for your use-case, could stay as a default implementation. However, we could also play a message broker and having msecs latency instead.\n\n\u003e \u003e - concurrent locking\n\u003e \n\u003e I believe for this simple use case this likely is on par with other solutions, see the locking prototype here: https://gerrit-review.googlesource.com/c/plugins/replication/+/241132\n\nIt depends on how many destinations you have. In our case we have tens of thousands with the same URI but different credentials and associated projects. Locking by URI would result in everyone stuck on the same lock.\n\nHaving something more granular, it would results in tens of thousands of directories that would eventually slow down the overall system.\n\n\u003e \u003e - stale file handles\n\u003e \n\u003e Aside from potentially interrupting code execution, these are likely not disruptive since they are effectively file not founds which can easily be handled.\n\nAck.\n\n\u003e \u003e - caching issues\n\u003e \n\u003e I believe that many of the current Gerrit NFS caching issues are due to the jgit caching layers which are hard to workaround.\n\u003e \n\u003e Without jgit in the way these should not be a big challenge. The files are currently immutable so they have no caching issues. In my experience refreshing parent directories will refresh any operation that uses directory listings on the child directory. It probably would be good (and easy) to build that into the plugin.\n\nWe tried that workaround, it works for your NFS implementation but did not for others. We cannot force a solution that would work only in very specific situations.\n\n\u003e \u003e - lack of real-time notifications\n\u003e \n\u003e I think you mean event vs polling (since real-time and Gerrit are not really compatible)? I address this in the next section.\n\nAck.",
      "parentUuid": "852d4965_c80197b5",
      "range": {
        "startLine": 282,
        "startChar": 54,
        "endLine": 283,
        "endChar": 41
      },
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c52d5e11_67dfb29c",
        "filename": "pages/design-docs/scaling-multi-master-replication/solution-distribute-replication-tasks-via-shared-FS.md",
        "patchSetId": 5
      },
      "lineNbr": 283,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-24T17:26:02Z",
      "side": 1,
      "message": "\u003e With NFS, the other nodes could see new files seconds or even minutes afterwards. Adding 1-2 mins delay to all existing replications could be a serious issue.\n\nNo matter how long the shared filesystem delay is, it should not be problematic for the use case where the same shared filesystem is being used to share the git repos since the git data will be limited by this delay also. I.E. there is no point knowing about a pending replication event on another node before the git objects for that event are available to replicate on the other node.\n\n\u003e In our use-case, we use HA and multi-site, we want to reduce the replication latency from tens of seconds to less than 1 sec: this solution won\u0027t be helpful.\n\nI would like to ask that you please keep in mind the scope of the review of this solution to my use case (which is not about reducing latency).\n\n\u003e Why not making the replication queue storage pluggable with a DynamicItem? The filesystem-based implementation, that would be best for your use-case, could stay as a default implementation. However, we could also play a message broker and having msecs latency instead.\n\nThis sounds like the domain of a follow on use case for those who want it (see \"Task distribution alternatives\" below). \n\nTechnical note: currently, I don\u0027t believe plugins can define their own DynamicItem without core registering the type as a DynamicItem. Since this would be a replication plugin type, core would not be aware of it to define it.\n\n\u003e \u003e \u003e - concurrent locking\n\u003e \u003e \n\u003e \u003e I believe for this simple use case this likely is on par with other solutions, see the locking prototype here: https://gerrit-review.googlesource.com/c/plugins/replication/+/241132\n\u003e \n\u003e It depends on how many destinations you have. In our case we have tens of thousands with the same URI but different credentials and associated projects. Locking by URI would result in everyone stuck on the same lock.\n\nPerhaps we are talking about different URIs than I am? I am talking about the URI in the current ReplicationTasksStorage.UriUpdate.uri object, perhaps you are talking about the entry in the replication.config file? My approach leads to the same level of locking that is currently seen with a single node. For my use case it is desirable to keep this level of locking in order to prevent having more masters from adding more load to destination resources than the load created by a single node.\n\n\u003e \u003e \u003e - caching issues\n\u003e \u003e \n\u003e \u003e I believe that many of the current Gerrit NFS caching issues are due to the jgit caching layers which are hard to workaround.\n\u003e \u003e \n\u003e \u003e Without jgit in the way these should not be a big challenge. The files are currently immutable so they have no caching issues. In my experience refreshing parent directories will refresh any operation that uses directory listings on the child directory. It probably would be good (and easy) to build that into the plugin.\n\u003e \n\u003e We tried that workaround, it works for your NFS implementation but did not for others. We cannot force a solution that would work only in very specific situations.\n\nOn second thought, I don\u0027t believe this is relevant since as I pointed out above, having replication coordination that is faster than git coordination does not provide any benefit to the use case being solved.",
      "parentUuid": "a96e654d_7d9e9ee7",
      "range": {
        "startLine": 282,
        "startChar": 54,
        "endLine": 283,
        "endChar": 41
      },
      "revId": "b6f79addb0d55e733c634722208983a1bda201ea",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    }
  ]
}