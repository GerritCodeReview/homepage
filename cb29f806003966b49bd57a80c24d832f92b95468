{
  "comments": [
    {
      "key": {
        "uuid": "f097d085_3e84cea1",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 1003873
      },
      "writtenOn": "2019-10-01T15:02:40Z",
      "side": 1,
      "message": "Will this be continued with a proposed new design?",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "af17167e_112b8b09",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 1002666
      },
      "writtenOn": "2019-10-01T17:05:46Z",
      "side": 1,
      "message": "Yes, Martin should be uploading it in the next day or so.",
      "parentUuid": "f097d085_3e84cea1",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "3ac853e8_2601730c",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-01T17:41:36Z",
      "side": 1,
      "message": "I am trying to ensure that the problem description stands on its own and is well described, understood, and agreed upon before offering solutions. As a community we noticed that as positive engineers, we tend to talk about solutions and often forget to describe the objective/problem and scope. This doc is intended to not have solutions. I will upload a follow on change with a proposal for a solution, and maybe someone else will have a solution.",
      "parentUuid": "af17167e_112b8b09",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "53a219fa_b7c00217",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-01T17:49:55Z",
      "side": 1,
      "message": "How many replication endpoints you have? What are the limits you are experiencing? Can you share some numbers?\n\nOn GerritHub, for instance, we have 40k replication endpoints and the replication plugin works as expected. However, the design is far from being perfect, so I do agree that a redesign is needed and I am happy to contribute.",
      "range": {
        "startLine": 11,
        "startChar": 21,
        "endLine": 11,
        "endChar": 33
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "cee26ebe_c1b019b1",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-01T18:22:39Z",
      "side": 1,
      "message": "I suspect that you mean something different by endpoints than what I mean by repliation destinations. By endpoints, it sounds like you mean combinations of URLs (i.e. replication config URLs x # of projects) after expanding for projects? I will update the doc to reflect what I mean. By destinations, I mean URI entries in the replication.config, we have ~6 destinations with ~7 threads each and we have a middle tier, since the master could not handle more destinations. The middle tier is another master that handles replicating to less important sites (more than 6 more) with a greater delay than the master. This middle tier was added years ago since we reached our site limit a long time ago. Now we want to add more sites to the main masters (sites with lower latency than the middle tier) and we can\u0027t, which is a shame since we have more than one master, but doing so would increase the overall counts thread (and thus run into the limits below), or reduce the thread counts to the existing sites and thus impact them. I will add some numbers to the doc also, since it might help illustrate the problem better.\n\nYou said that the design is far from perfect, it would be valuable to us if you could describe specific issues you encounter with the current design.",
      "parentUuid": "53a219fa_b7c00217",
      "range": {
        "startLine": 11,
        "startChar": 21,
        "endLine": 11,
        "endChar": 33
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4fc51ef6_cdbf8440",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-01T22:23:45Z",
      "side": 1,
      "message": "\u003e I suspect that you mean something different by endpoints than what I mean by repliation destinations. \n\nTrue, see below the number of destinations I have:\n  $ grep \u0027\\[remote \u0027 replication.config  | wc -l\n  9228\n\nThey are less than 40k, but still considerable.\n\n\u003e You said that the design is far from perfect, it would be valuable to us if you could describe specific issues you encounter with the current design.\n\nIt is basically not reliable and non-deterministic. Upon a ref-update, it may take for exactly the same repo:\n- a few seconds\n- a few minutes\n- never start\n\nThat is mainly because it tries to solve too many problems at once:\n- multiplexing of git events to replication events\n- aggregation of events in batches\n- queueing with persistent backend\n\nBy separating it into different concerns and taking some \"off-the-shelf\" pluggable components for some of those problems (e.g. persistent event queue), the code-base would become simpler and more flexible also.",
      "parentUuid": "cee26ebe_c1b019b1",
      "range": {
        "startLine": 11,
        "startChar": 21,
        "endLine": 11,
        "endChar": 33
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "efbbc38c_37921387",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-01T22:59:22Z",
      "side": 1,
      "message": "\u003e True, see below the number of destinations I have:\n\u003e   $ grep \u0027\\[remote \u0027 replication.config  | wc -l\n\u003e   9228\n\nI see, I suspect this paints a very different picture from how most of us use replication. Are each of those destinations effectively different customers? Does each of those destinations only replicate a small subset of projects to it? I believe each of those \"remote\" entries has to be served by at least one thread? I can start to see why things would be very unpredictable in your setup since you likely have a configuration that does not protect your server from being overloaded if all your customers made simultaneous updates. I believe you likely have immensely over configured your server already and are getting lucky that it is not crashing!\n\nIn our case our destinations replicate every project (all 12K+) on the server. So the scale of each of our destinations is very different than yours, effectively along a different axis. We have tons of project that we need to send to a few places, you likely have something close to each project needing to be individually sent to a separate destination? I am not sure that a common solution would help us both.",
      "parentUuid": "4fc51ef6_cdbf8440",
      "range": {
        "startLine": 11,
        "startChar": 21,
        "endLine": 11,
        "endChar": 33
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "11660854_8662fde8",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-01T23:07:35Z",
      "side": 1,
      "message": "\u003e \u003e True, see below the number of destinations I have:\n\u003e \u003e   $ grep \u0027\\[remote \u0027 replication.config  | wc -l\n\u003e \u003e   9228\n\u003e \n\u003e I see, I suspect this paints a very different picture from how most of us use replication. Are each of those destinations effectively different customers? \n\nMost of them, yes. Some organisation has hundreds of repos, other just a few.\nThe 40k repos are spread across 9k destinations.\n\nWe have then the multi-site destinations where we replicate *all of the repos* (3 sites at the moment).\n\n\u003e Does each of those destinations only replicate a small subset of projects to it? \n\nSee above: the repos are partitioned across destinations.\n\n\u003e I believe each of those \"remote\" entries has to be served by at least one thread? \n\nYes.\n\n\u003e I can start to see why things would be very unpredictable in your setup since you likely have a configuration that does not protect your server from being overloaded if all your customers made simultaneous updates.\n\nPossible, not very likely. We are HA and multi-site anyway, so to bring down the service they need to kill *all of them*.\n\n\u003e I believe you likely have immensely over configured your server already and are getting lucky that it is not crashing!\n\nIt has crashed (actually just this afternoon) but not on all the sites at the same time.\nSo, I am interested in your design effort :-)\n\n\u003e In our case our destinations replicate every project (all 12K+) on the server. So the scale of each of our destinations is very different than yours, effectively along a different axis. We have tons of project that we need to send to a few places, you likely have something close to each project needing to be individually sent to a separate destination? I am not sure that a common solution would help us both.\n\nWe have that use-case also. As we will increase the number of nodes and sites, we\u0027ll have exactly the same problem as you have :-) So, again, we have a common interest here.",
      "parentUuid": "efbbc38c_37921387",
      "range": {
        "startLine": 11,
        "startChar": 21,
        "endLine": 11,
        "endChar": 33
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5658d9b1_c2a70ff1",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-01T23:42:31Z",
      "side": 1,
      "message": "\u003e \u003e I see, I suspect this paints a very different picture from how most of us use replication. Are each of those destinations effectively different customers? \n\u003e \n\u003e Most of them, yes. Some organisation has hundreds of repos, other just a few.\n\u003e The 40k repos are spread across 9k destinations.\n\u003e \n\u003e We have then the multi-site destinations where we replicate *all of the repos* (3 sites at the moment).\n\n\nOK, makes sense, thanks for that picture.\n\n\u003e \u003e I believe each of those \"remote\" entries has to be served by at least one thread? \n\u003e \n\u003e Yes.\n...\n\u003e It has crashed (actually just this afternoon) but not on all the sites at the same time.\n\u003e So, I am interested in your design effort :-)\n\nI suspect that you could benefit from a much simpler solution. Perhaps to help you out, it would make sense for the replication plugin to put all the remotes without a \u0027threads \u003d\u0027 entry into a common thread pool that you can then configure the thread count on. Or more flexibly, make the plugin support a \u0027thread-pool \u003d XXX\u0027 key that could point to a \u0027[thread-pool \"XXX\"]\u0027 block that would have a \u0027thread \u003d x\u0027 entry in it? That should allow you to set more reasonable limits on the 9K destinations by putting most of them in a common thread pool or two with fairly low thread counts. Both of those suggestions  would be fairly easy to implement given the structure of the current plugin and queuing.\n\n\n\u003e We have that use-case also. As we will increase the number of nodes and sites, we\u0027ll have exactly the same problem as you have :-) So, again, we have a common interest here.\n\nYes, I agree. To be clear though, I do not want to address with this design-doc the problem of having a minimum of one thread dedicated per \"remote\" block.",
      "parentUuid": "11660854_8662fde8",
      "range": {
        "startLine": 11,
        "startChar": 21,
        "endLine": 11,
        "endChar": 33
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "db70cdcc_03b9e6fa",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 11,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-02T00:03:58Z",
      "side": 1,
      "message": "I have added some number to PS3, please let me know if that is not enough info.",
      "parentUuid": "5658d9b1_c2a70ff1",
      "range": {
        "startLine": 11,
        "startChar": 21,
        "endLine": 11,
        "endChar": 33
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "78dde81a_32338ced",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 22,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-01T17:49:55Z",
      "side": 1,
      "message": "Do you want to improve the reliability also? Or only the scalability?",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "66804a65_39ae5ca4",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 22,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-01T18:22:39Z",
      "side": 1,
      "message": "This doc is intended to address scalability only, things seem to be fairly reliable for us on the master side. I believe our master side reliability comes from adapting our master usage to be inline with the limits described below. When we tried to exceed those limits years ago we had master issues. I believe that reliability issues that we do run into now tend to be bugs (mostly on the receiving side with git), not architecture.",
      "parentUuid": "78dde81a_32338ced",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e8d0ed23_aa2be059",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 22,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-01T22:23:45Z",
      "side": 1,
      "message": "See above my comments on reliability: yes it is all about bugs and I\u0027ve been fixing many of them recently :-)\n\nHowever, when a piece of software has so many bugs, it typically means that its complexity is too high for humans.",
      "parentUuid": "66804a65_39ae5ca4",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "deffbc39_9c002f64",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 22,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-01T23:42:31Z",
      "side": 1,
      "message": "Thank you for having greatly simplified the persistent storage recently.",
      "parentUuid": "e8d0ed23_aa2be059",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9ff1ab36_0a85feda",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 72,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-01T17:49:55Z",
      "side": 1,
      "message": "This is not related to the use-cases though. Would you add the resiliency part to the use-cases?",
      "range": {
        "startLine": 71,
        "startChar": 57,
        "endLine": 72,
        "endChar": 21
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "fb930519_bf7c09c4",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 72,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-01T18:22:39Z",
      "side": 1,
      "message": "I believe it is because we want to add more masters to support more destinations, and with the current design adding more masters makes this problem worse, not better.",
      "parentUuid": "9ff1ab36_0a85feda",
      "range": {
        "startLine": 71,
        "startChar": 57,
        "endLine": 72,
        "endChar": 21
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c93b257b_96242184",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 72,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2019-10-01T22:23:45Z",
      "side": 1,
      "message": "Oh yes, agreed.",
      "parentUuid": "fb930519_bf7c09c4",
      "range": {
        "startLine": 71,
        "startChar": 57,
        "endLine": 72,
        "endChar": 21
      },
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "aa3c3cb6_96120e27",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 78,
      "author": {
        "id": 1011123
      },
      "writtenOn": "2019-09-28T01:09:48Z",
      "side": 1,
      "message": "Should this line be prefixed with \"####\"?",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b7f592a1_e46adafb",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 78,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-02T00:03:58Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "aa3c3cb6_96120e27",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c8a52554_8f71ebf5",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 89,
      "author": {
        "id": 1011123
      },
      "writtenOn": "2019-09-28T01:09:48Z",
      "side": 1,
      "message": "Same here",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "99294619_77e1939f",
        "filename": "pages/design-docs/scaling-multi-master-replication/use-cases.md",
        "patchSetId": 3
      },
      "lineNbr": 89,
      "author": {
        "id": 1003883
      },
      "writtenOn": "2019-10-02T00:03:58Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "c8a52554_8f71ebf5",
      "revId": "cb29f806003966b49bd57a80c24d832f92b95468",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": false
    }
  ]
}